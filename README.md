Latent Sculpting for Zero-Shot GeneralizationA Manifold Learning Approach to Out-of-Distribution Anomaly DetectionThis repository contains the official PyTorch implementation of the research paper: "Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection".üöÄ OverviewStandard deep learning classifiers often suffer from "Generalization Collapse" when facing zero-shot Out-of-Distribution (OOD) anomalies. This project introduces a Hierarchical Two-Stage Framework to address this limitation:Stage 1 (Latent Sculpting): A hybrid 1D-CNN + Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL). This stage actively "sculpts" benign traffic into a low-entropy, hyperspherical cluster while pushing known anomalies away.Stage 2 (Probabilistic Expert Review): A Masked Autoregressive Flow (MAF) trained exclusively on the structured benign manifold to learn an exact density estimate.üìä Key Results (CIC-IDS-2017)ModelKnown Attacks (F1)Zero-Shot / Unseen Attacks (F1)Infiltration Detection RateSupervised MLP Baseline0.980.300.00%Unsupervised OCSVM0.760.7685.71%Proposed Two-Stage Method0.960.8788.89%üõ†Ô∏è Prerequisites & Setup (Google Colab)This project is optimized for Google Colab using a High-RAM runtime.1. DependenciesThe code relies on standard PyTorch and Data Science libraries.pip install torch torchvision torchaudio pandas numpy scikit-learn matplotlib seaborn tqdm umap-learn joblib pyarrow
2. Dataset PreparationThis project uses the CIC-IDS-2017 dataset. Due to size constraints, the CSV files are not included in this repo.Download the machine learning CSV files from the UNB Dataset Page.Upload the following files to your Colab environment:Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csvFriday-WorkingHours-Morning.pcap_ISCX.csvMonday-WorkingHours.pcap_ISCX.csvFriday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csvTuesday-WorkingHours.pcap_ISCX.csvThursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csvWednesday-workingHours.pcap_ISCX.csvThursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csvüèÉ UsageThe pipeline is split into two sequential scripts to manage memory efficiently.Step 1: Preprocessing (preprocess.py)Loads raw CSVs, cleans data (Infinity/NaN removal), performs feature engineering (Bytes/Packet, Packets/Sec), and splits data into Seen (Training) and Unseen (Zero-Shot) sets.Output: Generates seen_data.feather, unseen_data.feather, and scaler_and_cols.joblib.# Run in Colab cell
!python preprocess.py
Step 2: Training & Evaluation (train_and_evaluate.py)Loads the processed artifacts, trains the Hybrid Encoder (Stage 1), trains the MAF (Stage 2), and performs the final evaluation.Stage 1: Asymmetric balancing is applied (Benign samples undersampled to match largest Anomaly class).Stage 2: Calculates dynamic thresholds at 99th, 97th, and 95th percentiles.Evaluation: Generates AUROC, AUPRC, and per-attack recall tables.# Run in Colab cell
!python train_and_evaluate.py
üß† Model ArchitectureStage 1: Hybrid EncoderInput: 71 features (Zero-Variance filtered).CNN Front-End: 5-layer 1D-CNN for local feature extraction ($k=2$).Transformer Back-End: 3-layer Transformer Encoder for global context ($N=4$ heads).Loss: DCCL (Compactness + Separation).Stage 2: Density EstimatorModel: Masked Autoregressive Flow (MAF).Depth: 16 MADE layers.Hidden Dimension: 512.üìÑ CitationIf you use this code or methodology in your research, please cite our paper:@misc{chhetri2025latent,
  title={Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection},
  author={Chhetri, Rajeeb Thapa and Thapa, Saurab and Chen, Zhixiong},
  year={2025},
  eprint={Pending},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
ü§ù AcknowledgementThis research was partially funded by the U.S. Department of Homeland Security (DHS).We acknowledge the use of the CIC-IDS-2017 dataset provided by the Canadian Institute for Cybersecurity.Maintained by Rajeeb Thapa Chhetri
